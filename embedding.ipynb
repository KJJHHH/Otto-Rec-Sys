{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch.version\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.cuda.is_available(), torch.version.cuda, torch.__version__\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "trainfile = \"train.jsonl\"\n",
    "batch_size = 100\n",
    "start = 0\n",
    "\n",
    "def loadBatchData(start, batch_size, filename):\n",
    "    trainset = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for id, line in enumerate(file):\n",
    "            if id < start:\n",
    "                continue\n",
    "            if id >= start + batch_size:\n",
    "                break\n",
    "            try:\n",
    "                data = json.loads(line)  # Convert JSON string to dictionary\n",
    "                trainset.append(data)\n",
    "            except: \n",
    "                print(\"bad lines\")\n",
    "                continue\n",
    "            \n",
    "    return trainset\n",
    "\n",
    "# define action space\n",
    "def getItems(trainset) -> set:\n",
    "    actions = [] # items\n",
    "    for session in trainset:\n",
    "        for i in [i[\"aid\"] for i in session[\"events\"]]:\n",
    "            actions.append(i)\n",
    "    return set(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "- one hot encoding too waste space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoding():\n",
    "    def __init__(self, trainset: list):\n",
    "        self.trainset = trainset\n",
    "        self.encoding = None\n",
    "        self.items_len = 0\n",
    "        \n",
    "        \n",
    "    def getMapping(self) -> dict:\n",
    "        \"\"\"\n",
    "        Output:\n",
    "            key: item id\n",
    "            data: one hot encoding\n",
    "        \"\"\"\n",
    "        items = getItems(self.trainset)\n",
    "        self.items_len = max(items) + 1\n",
    "        data = np.array([list(items)])\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        one_hot = encoder.fit_transform(data)\n",
    "        \n",
    "        print(f\"num items: {self.items_len}\")\n",
    "        \n",
    "        self.encoding = {}\n",
    "        for i in encoder.get_feature_names_out():\n",
    "            encode = i.split(\"_\")\n",
    "            data = int(encode[0][1:])\n",
    "            key = int(encode[1])\n",
    "            self.encoding[key] = data  \n",
    "\n",
    "    def id2Vec(self, X: list, y: list) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \n",
    "        X_vec = []\n",
    "        y_vecs = []\n",
    "        \n",
    "        for x, i in zip(X, y):\n",
    "            # X\n",
    "            x_vec = [0 for i in range(self.items_len)]\n",
    "            x_vec[self.encoding[x]] = 1\n",
    "            X_vec.append(x_vec)\n",
    "            \n",
    "            # y\n",
    "            y_vec = [0 for i in range(self.items_len)]\n",
    "            for j in i:\n",
    "                y_vec[self.encoding[j]] = 1\n",
    "            y_vecs.append(y_vec)           \n",
    "            \n",
    "        return torch.tensor(X_vec), torch.tensor(y_vecs)\n",
    "    \n",
    "    def ySync(self, y: list) -> list:\n",
    "        max_len = 0 \n",
    "        for k in y:\n",
    "            if max_len < len(k):\n",
    "                max_len = len(k)\n",
    "                \n",
    "        for k in y:\n",
    "            if len(k) < max_len:\n",
    "                for i in range(max_len - len(k)):\n",
    "                    k.append(0)\n",
    "                    \n",
    "        return y\n",
    "    \n",
    "    def getEmbeddingTrainSet(self, context_window: int = 60*60*24*1000) -> tuple[list, list]:\n",
    "        \"\"\"\n",
    "        X[0]: [0 ... 0 1 0 ... 0] index at X[0] = 1\n",
    "        y[0]: [0...0 1 0...0 ] index at y[0] = 1\n",
    "        y[0] for compute loss\n",
    "        \n",
    "        Improve:\n",
    "        1. y duplicate for each timestamp?\n",
    "        \"\"\"\n",
    "        self.trainset[0]\n",
    "        context_window = 60*60*24*1000 # 1 day\n",
    "        X = [] # Input\n",
    "        y = [] # Near\n",
    "        for session in tqdm(self.trainset):\n",
    "            session_ = pd.DataFrame(session[\"events\"])\n",
    "            for ev in session[\"events\"]:\n",
    "                X.append(ev[\"aid\"])\n",
    "                y.append(list(session_[(session_[\"ts\"] < ev[\"ts\"] + context_window) & (session_[\"ts\"] > ev[\"ts\"] - context_window)][\"aid\"]))\n",
    "\n",
    "        y = self.ySync(y)\n",
    "        \n",
    "        # _, y = self.id2Vec(X, y)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.emb_size = emb_size \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num items: 1855501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 29.07it/s]\n"
     ]
    }
   ],
   "source": [
    "trainset = loadBatchData(start, batch_size, trainfile)\n",
    "\n",
    "encoding = Encoding(trainset)\n",
    "encoding.getMapping()\n",
    "X, y = encoding.getEmbeddingTrainSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 197/10681 [00:11<10:13, 17.09it/s]"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(X).to(device)\n",
    "y = [torch.tensor(i).to(device) for i in y]\n",
    "\n",
    "vocab_size = max(X) + 1\n",
    "emb_size = 10\n",
    "init_train = True\n",
    "embed_path = \"embed.pt\"\n",
    "\n",
    "if init_train:\n",
    "    embedding = Embedding(vocab_size, emb_size).to(device)\n",
    "else:\n",
    "    torch.load(embed_path, map_location=lambda storage, loc: storage, weights_only=True)\n",
    "optimizer = optim.Adam(embedding.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR every 10 epochs\n",
    "\n",
    "embedding.train()\n",
    "for i in range(3):\n",
    "    total_loss = []\n",
    "    for i, x in enumerate(tqdm(X)):\n",
    "        optimizer.zero_grad()\n",
    "        embedded = embedding(x)\n",
    "        loss = torch.tensor(0)\n",
    "        for j in y[i]:\n",
    "            if j != 0:\n",
    "                loss = loss - embedded[j]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    scheduler.step()\n",
    "    print(np.mean(total_loss))\n",
    "    torch.save(embedding.state_dict(), embed_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
