{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.version\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing.managers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def loadStartSes(loadinfo_file):\n",
    "    if os.path.exists(loadinfo_file):\n",
    "        ses = []   \n",
    "        with open(loadinfo_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                ses.append(json.loads(line))\n",
    "                \n",
    "        return max(ses) + 1\n",
    "    return 0   \n",
    "\n",
    "def loadBatchData(start, batch_size, filename) -> list:\n",
    "    trainset = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for id, line in enumerate(file):\n",
    "            if id < start:\n",
    "                continue\n",
    "            if id >= start + batch_size:\n",
    "                break\n",
    "            try:\n",
    "                data = json.loads(line)  \n",
    "                trainset.append(data)\n",
    "            except: \n",
    "                print(\"bad lines\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "    return trainset\n",
    "\n",
    "def loadNumItems():\n",
    "    with open(\"max_item_id.json\", \"r\") as f:\n",
    "        return json.loads(f.read())[\"max_item_id\"] + 1\n",
    "\n",
    "def storeNumItems(items):\n",
    "    with open(\"max_item_id.json\", \"w\") as f:\n",
    "        f.write(json.dumps({\"max_item_id\": max(items)}))\n",
    "\n",
    "def getItems(filename:  str) -> set:\n",
    "    items = set()\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in tqdm(file): \n",
    "            session = json.loads(line)  \n",
    "            for ev in session[\"events\"]:\n",
    "                items.add(ev[\"aid\"])\n",
    "    \n",
    "    storeNumItems(items)\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "# Embedding model\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.emb_size = emb_size \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numTotalSessions():\n",
    "    sum = 0\n",
    "    with open(\"train.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in tqdm(file):\n",
    "            sum += 1\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load set\n",
    "trainfile = \"train.jsonl\"\n",
    "loadinfo_file = \"loadinfo_file.json\"\n",
    "batch_size = 1000\n",
    "batch_cnt = 0\n",
    "start_ses = 0\n",
    "\n",
    "# Train set  \n",
    "epoch = 1\n",
    "vocab_size = loadNumItems()\n",
    "emb_size = 10\n",
    "embedding_model_path = \"embed.pt\"\n",
    "trainbatch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 628/628 [01:50<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session start: 1000 Epoch: 10 | Loss: 675.912981944479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [01:31<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session start: 2000 Epoch: 106 | Loss: 637.2207468580164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 328/532 [01:00<00:27,  7.37it/s]"
     ]
    }
   ],
   "source": [
    "model = Embedding(vocab_size, emb_size).to(device)\n",
    "if os.path.exists(embedding_model_path):\n",
    "    model.load_state_dict(torch.load(embedding_model_path, map_location=lambda storage, loc: storage, weights_only=True))\n",
    "\n",
    "\n",
    "def taskGetNeighbor(\n",
    "    session: list,\n",
    "    context_window: int = 60*60*24*1000, \n",
    "    loadinfo_file = \"loadinfo_file.json\"\n",
    "    ) -> None:\n",
    "    \n",
    "    session_ = pd.DataFrame(session[\"events\"])\n",
    "    for ev in session[\"events\"]:\n",
    "        X.append(ev[\"aid\"])\n",
    "        y.append(list(session_[(session_[\"ts\"] < ev[\"ts\"] + context_window) & (session_[\"ts\"] > ev[\"ts\"] - context_window)][\"aid\"]))\n",
    "    \n",
    "    with open(loadinfo_file, \"a\") as f:\n",
    "        f.write(json.dumps(session[\"session\"]) + \"\\n\")\n",
    "        \n",
    "    return {session[\"session\"]: [X, y]}\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Load data\n",
    "    start_ses = loadStartSes(loadinfo_file)\n",
    "    trainset = loadBatchData(start_ses, batch_size, trainfile)\n",
    "    start_ses += batch_size\n",
    "    if trainset == []:\n",
    "        break    \n",
    "    \n",
    "    # Get data for train\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        X = manager.list([]) # SessionID\n",
    "        y = manager.list([]) # Near SessionID\n",
    "\n",
    "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "            result = pool.map(taskGetNeighbor, trainset)\n",
    "            \n",
    "        X = torch.tensor(list(X))\n",
    "        y = [torch.tensor(lst) for lst in list(y)]\n",
    "        \n",
    "        pool.close()\n",
    "        \n",
    "    traindata = EmbDataset(X)\n",
    "    trainloader = DataLoader(traindata, batch_size=trainbatch_size, shuffle=True, num_workers=4)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.8)  # Reduce LR every 10 epochs\n",
    "    \n",
    "    model.train()   \n",
    "    for i in range(epoch):\n",
    "        total_loss = []\n",
    "        ses_cnt = 0\n",
    "        \n",
    "        for x in tqdm(trainloader):\n",
    "            x = x.to(device)\n",
    "            y = [i.to(device) for i in y]\n",
    "            loss = torch.tensor(0.).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            embedded = model(x)\n",
    "            for i, emb in enumerate(embedded):\n",
    "                loss -= torch.sum(emb[y[ses_cnt*trainbatch_size + i]])\n",
    "            loss = loss / (i + 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "            ses_cnt += 1\n",
    "            \n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), embedding_model_path)\n",
    "    \n",
    "        print(f\"Session end: {start_ses} Epoch: {i} | Loss: {np.mean(total_loss)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreading?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
